# Demo App

This application compares latency between LLM API endpoints and local AI.

## LLM container

I'm using a pre-built Ollama container. Note that this build is excluded from the CI because it triggers a rebuild with every commit to main.

For more prebuilt containers, [go here.](https://github.com/mischavandenburg/ollama-containers)


